PROJECT OVERVIEW

My project runs a fluid simulation and outputs the simulation to a gif. I am not a physics expert and I
ported some C code from this source https://mikeash.com/pyblog/fluid-simulation-for-dummies.html to golang
to handle the fluid simulation portion of the project. Mike Ash's blog post does a very good job of
explaining how a fluid simulation works at a high level. My project 3 runs in a similar way to project 2. It
reads in json from stdin and outputs the results of the simulation with the specified filename. I have provided
some sample input files (test.txt + test_extra.txt) within the src directory.


PROJECT STRUCTURE

There are 4 main packages (+ a testing package simpletest which I used at the beginning but should be ignored).
They are driver (run the program), barrier (a barrier implementation using channels), gif (handles all gif 
related code, saving, writing etc.) and fluid (the fluid simulation + data/ functional decomposition). When
you run the program from driver almost all the work takes place inside fluid. The physics portion of the fluid
simulation occurs within fluid/fluid.go within the FluidCube struct. fluid/simulation.go adds Simulation and SimulationGIF
structs. Simulation contains information about how to update the fluid cube every tick. SimulationGIF contains
both Simulation and a GIF members. fluid/parallel.go contains task and subtask structs and functions.
The data decomposition approach used in parallel.go is similar to basic approach I used in proj2. I added some
functional decomposition components for this project.


MY PARALLEL SOLUTION

There are two different components to my parallel solution. The gif writing portion of the code consists of a
refactored and improved adaptation of my proj2 code. For each input json object, a task is generated by the
main goroutine. Then within each task, each "frame" (sub-image) within the gif is split into chunks and written
by a user specified number of goroutines. You can learn more about how to specify the number of threads in the
README.

Fluid simulation is inherently sequential as each "tick" in the simulation depends on the previous tick. As a
result, this part of the simulation cannot be parallelized easily. However, what can be parallelized is updating
the current tick of the simulation and copying the values of the previous tick to a gif frame. This does not
happen by default, if you want to enable this optimization use the -bsp flag. (See the README for more info.)
To enable this optimization I created my own barrier implementation (ChanBarrier) which uses channels under the
hood.


CHALLENGES

One of the main challenges I faced was displaying the fluid simulation. I wasn't able to realistically add
dye/velocity to the simulation and display it in an image so that the motion looks natural. Due to time
constraints and because it produced an interesting effect when dye interacts with nearby dye,
I settled on adding dye randomly within a fluid simulation, hence why there is only one configurable
simulation type (see CREATING YOUR OWN TESTS section below for more info).
If I could figure out this problem I think the output gif would be much more interesting and I
wrote my code in such a way that other simulation types can be added in the future.

I chose fluid simulation because I think the results look visually very impressive and because I am interested
in how fluid simulations work. I chose gifs because they are fun to look at, a lot of what I learned in project 2
is applicable to gifs and because of standard library support. I thought the two are an interesting combination
because writing an image is an embarassingly parallel problem while running a fluid simulation is inherently
sequential. Structuring my code in a such a way that enabled a large amount of parallelism was an interesting and
challenging problem. I ultimately decided that the bulk synchronous parallel model was appropriate.


MY TESTING MACHINE

OS: Linux Mint 19.3
Linux Kernel: 5.3.0
Processor: Intel Core i5-8265 CPU
Cores: 4 x 1.60Ghz
Memory: 8GB


MY GRAPH

The graph (speedup_graph.png) shows what we expect given what we know about Amdahl's law. The speedup is significant
but it starts to taper off the more threads you throw at the problem. My CPU could also be a factor in the tapering off.
I only have 4 cores and hyper threading is probably enabled but I don't know for sure. As is expected, turning on bsp mode
results a modest but consistent speedup which is nice to see. (I wasn't sure at first if there would even be any bsp mode speedup
as there is some extra work that needs to be done copying the density information into a temporary buffer so the simulation isn't 
overriding values that the gif writer is reading from.)


CREATING YOUR OWN TESTS

I have provided two sample input files (test.txt + test_extra.txt) within the src directory. If you want to add
additional tests these are the configurable fields in the json object:

           size 	  : int       // size of the simulation, e.g. size=200 will produce a 200*200 pixel gif
           frames 	  : uint      // number of frames in the gif (how long to run the simulation for)   
           simType    : string    // type of simulation, only one type supported: "random". I would've liked to add more types but I didn't have time
           outPath    : string    // the name/ path of the output gif
[Optional] diffusion  : float32   // how fast stuff spreads out in the fluid
[Optional] viscosity  : float32   // how thick the fluid is
[Optional] delay 	  : int       // the delay between frames, measured in 100ths of a second
[Optional] repeat	  : int	      // how many times to repeat the simulation update() function each tick
[Optional] fadeOut	  : bool	  // stop adding dye for the last 50 ticks (let the existing dye fade out)

Add this to a text file (all on one line) then run the code e.g. `go run src/driver/driver.go < yourtestfile.txt`


HOTSPOTS/ BOTTLENECKS

As mentioned elswhere, the greatest bottleneck the fluid simulation. The greatest hotspots are the fluid
simulation and writing gif frames. Only writing gif frames is really sped up in the parallel version. Each
frame is split into chunks which are written by many goroutines. Updating the fluid simulation is sequential
due to each tick relying on the previous one. Computations within the FluidCube.Step() function could probably
be parallelized but this has its own problems, the code is very complicated and it's not very obvious how you
could break up the problem and merge the results while not changing the outcome. The merging step would need
to be very sophisticated.


ADDITIONAL QUESTIONS

What limited your speedup? Is it a lack of parallelism? (dependencies) Communication or synchro-
nization overhead?

My speedup is limited by the sequential fluid simulation problem. There is a data dependency here which inhibits
parallelism. I tried to reduce unecessary sequential code where possibly, splitting up tasks into subtasks (data
decomposition) and overlapping image writing and simulation updates with bsp mode (functional decomposition).
With bsp mode the use of a barrier isn't ideal. The barrier stops all the waiting goroutines from doing any useful
work and there is probably a smarter (if not harder to reason about) way to handle this problem. I just thought the
bsp approach was one interesting way to handle the particular problem. Overall with splitting tasks into subtasks
I tried to maximize the amount of parallelism. Tasks can be handled simultaneously and threads should be busy most
of the time. Inter-thread communication is not a big problem in my code, synchronization via barriers or buffered
channels is the main parallelism inhibitor.
